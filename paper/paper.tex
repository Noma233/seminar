\documentclass[ams, a4j]{U-AizuGT}
\usepackage{pifont}
% \usepackage{graphicx}
\usepackage{cite}
\usepackage{mathtools}
% \usepackage{listings,jvlisting}
% \documentclass[a4j]{jarticle} %ここは関係ない
\usepackage{listings,jvlisting} 
\usepackage[dvipdfmx]{graphicx}
\usepackage{physics}
\bibliographystyle{ieicetr}



\lstset{
    frame=single,
    numbers=left,
    tabsize=2
}


% \usepackage{listings,jlisting}

% \lstset{%
%   language={C},
%   basicstyle={\small},%
%   identifierstyle={\small},%
%   commentstyle={\small\itshape},%
%   keywordstyle={\small\bfseries},%
%   ndkeywordstyle={\small},%
%   stringstyle={\small\ttfamily},
%   frame={tb},
%   breaklines=true,
%   columns=[l]{fullflexible},%
%   numbers=left,%
%   xrightmargin=0zw,%
%   xleftmargin=3zw,%
%   numberstyle={\scriptsize},%
%   stepnumber=1,
%   numbersep=1zw,%
%   lineskip=-0.5ex%
% }
\lstset{
  basicstyle={\ttfamily},
  identifierstyle={\small},
  commentstyle={\smallitshape},
  keywordstyle={\small\bfseries},
  ndkeywordstyle={\small},
  stringstyle={\small\ttfamily},
  frame={tb},
  breaklines=true,
  columns=[l]{fullflexible},
  numbers=left,
  xrightmargin=0zw,
  xleftmargin=3zw,
  numberstyle={\scriptsize},
  stepnumber=1,
  numbersep=1zw,
  lineskip=-0.5ex
}

% \bibliographystyle{ieice}
\author{Ryuki Hiwada}
\studentid{s1280076}
\supervisor{Naohito Nakasato}

\title{Domain Specific Language for high performance computing}

\begin{document}

\maketitle
\begin{abstract}

\end{abstract}
\section{Introduction}


\subsection{Background}

宇宙物理学や流体力学等、様々な分野で粒子系シミュレーションが行われている.
例えば、惑星間の重力による力学的進化をシミュレーションしたN体シミュレーションや
水の流体運動のモデル化がある。
これらのシミュレーションには粒子の数に対して計算量が膨大になる。
例としてN体シミュレーションの場合を説明する。N体シミュレーションでは,
ほとんどの計算時間を重力相互作用の計算が占めている。\eqref{eq:gravity} は、
重力相互作用の式である.
\begin{equation}
a_i = \sum_{\substack{j \neq i}}^{N} G m_j \frac{\vb{x}_j - \vb{x}_i}{\left( \left| \vb{x}_j - \vb{x}_i \right| + \epsilon^2 \right)^{3/2}\label{eq:gravity}}
\end{equation}

ここで $a_i$, $\vb{x}_i$はそれぞれ粒子iの加速度と位置を表し、
$m_j$,$\vb{x}_j$はそれぞれ粒子jの質量と位置を表し、
$N$,$G$,$\epsilon$はそれぞれ粒子の数、重力定数発散を防ぐためのソフトニングパラメータである。
\eqref{eq:gravity}から計算量は粒子数Nに比例して、\begin{math}O(N^2) \end{math}となる。
N体シミュレーションでは、粒子の数が膨大になる場合が多い。例えば、
球状星団という天体には星が約$10^4 ~ 10^5 $ 個存在する。このため非常に長い計算時間がかかるため高速化が求めれる。
粒子間相互作用の計算は、粒子同士の計算が独立しているためデータ並列性が高い。そのため、単一の命令に対して複数の
データを処理するSIMDによる並列に適している。
SIMDによる並列計算の例として、小池らが行ったGRAPE-DR\cite{GRAPE-explain-thesis}による重力多体問題シミュレーションがある。\cite{GRAPE-DR-thesis}
しかし、SIMD計算を行うには,コンピュータのアーキテクチャを考慮したコードを記述しなくてはならない。

この研究の目的は、ユーザーが粒子間相互作用の計算を並列化を意識することなく、並列に実行できるように
することである。そのためにPykerというDomain Specific Language\lparen DSL \rparen を開発した。
Pykerでは、ユーザに必要な変数の定義と数式から相互作用の計算をSIMD命令によって並列化したコードを生成する。
Pykerの評価方法としては、C言語のコンパイラが自動で行うSIMD化とPykerによるSIMD化の性能を比較する。
そのために、3つの相互作用の計算を行うコードをそれぞれにPykerから生成されたSIMD命令を適応しない
場合のコードと、SIMD命令を適応したコードの実行時間を計測し比較する。
また、Pykerを作成するにあたって粒子間相互作用の計算するコードを生成するPIKGを参考にした。




\section{Construction}
We use Sympy, a library in Python for implementing  the implementation of pyker.
SymPy supports various operations, and the formulas using these operations are internally treated as 
syntax trees. We leveraged this functionality to read formulas and generate code capable of parallel execution.
For parallelization, we used an instruction set named Advanced Vector Extensions 2 \lparen AVX2\rparen. AVX2 is one of the 
extended instruction sets implemented in Intel's CPUs. The next section will explain how parallelization is carried out using AVX2.


\subsection{AVX2 for parallelization}
% _mm256dの説明はここですべきか？ codegen で説明すべきか
To perform SIMD operations, just like with regular computations, data is first loaded from memory, computed based on that data,
and the results are stored. However, in the case of SIMD, computations are performed using SIMD-specific registers, so the 
methods of loading, computing, and storing differ. When loading, multiple elements that will undergo the same instruction are
stored in a SIMD-specific register. These are then used to perform computations simultaneously, and the results are stored for
each. 


For instance, consider a calculation like C = X + Y, where elements of double-precision floating arrays x and y are computed
. When using AVX2 for SIMD, since the registers are 256 bits, four elements from x and y are loaded each. These are then added 
simultaneously, and the results are stored in the array C. When loading into SIMD registers, if it's a 1-dimensional array, 
contiguous memory access is sufficient, but for data structures like Array of Structures \lparen AoS \rparen shown in Figure a., elements need
to be loaded individually. 


For example, when considering the 3-dimensional coordinates of particles as double-precision floats 
in pos\lbrack n\rbrack \lbrack 3\rbrack  \lparen where n is the total number of particles, and pos\lbrack i \rbrack \lbrack 0\rbrack , pos\lbrack i\rbrack \lbrack 1\rbrack , 
pos\lbrack i\rbrack \lbrack 2\rbrack  represent the x, y, z coordinates 
of the i-th particle, respectively\rparen, and calculating the difference in x coordinates between particles i and j, it becomes dx = 
pos\lbrack i\rbrack \lbrack 0\rbrack  - pos\lbrack j\rbrack \lbrack 0\rbrack . However, for parallelization with SIMD, desiring dx0 = pos\lbrack i\rbrack \lbrack 0\rbrack  - pos\lbrack j \rbrack \lbrack 0\rbrack , dx1 = pos\lbrack i+1\rbrack \lbrack 0\rbrack  - pos\lbrack j\rbrack \lbrack 0\rbrack 
, dx2 = pos\lbrack i+2\rbrack \lbrack 0\rbrack  - pos\lbrack j\rbrack \lbrack 0\rbrack , dx3 = pos\lbrack i+3\rbrack \lbrack 0\rbrack  - pos\lbrack j\rbrack \lbrack 0\rbrack , 
the data are packed as \lparen pos\lbrack i\rbrack \lbrack 0\rbrack , pos\lbrack i+1\rbrack \lbrack 0\rbrack , pos\lbrack i+2\rbrack \lbrack  0\rbrack , pos\lbrack  i+3\rbrack \lbrack  0\rbrack \rparen,
\lparen pos\lbrack  j\rbrack \lbrack  0\rbrack , pos\lbrack  j\rbrack \lbrack  0\rbrack , pos\lbrack  j\rbrack \lbrack  0\rbrack , pos\lbrack  j\rbrack \lbrack  0\rparen, \lparen dx0, dx1, dx2, dx3\rparen and subtraction is performed simultaneously. However, since data needs
to be loaded onto SIMD registers all at once, the gather instruction is used to load data for particle i. The gather instruction is used
to read non-contiguous data elements from memory addresses by specifying addresses. An explanation of the gather instruction is shown in 
Figure x. Data loaded in this way are computed simultaneously using SIMD instructions. 

\begin{lstlisting}[frame=single]
  dx0 = pos[i][0] - pos[j][0]
  dx1 = pos[i + 1][0] - pos[j][0]
  dx2 = pos[i + 2][0] - pos[j][0]
  dx3 = pos[i + 3][0] - pos[j][0]
\end{lstlisting}

\begin{lstlisting}[frame=single]
  (x1, x2, x3, x4) + (y1, y2, y3, y)

\end{lstlisting}


When storing computation results, addresses may 
not be contiguous. In such cases, there is an instruction called scatter, which is the counterpart to gather, but it is not supported 
in AVX2, so data are stored using pointers individually. In this way, parallelization is achieved with SIMD. Moreover, these instructions
can be explicitly handled in high-level programming languages like C and C++ using intrinsic functions. To handle intrinsic functions 
in C and C++, include the file immintrin.h, which is available as a standard in the language. An example of C++ code that calculates the difference in x coordinates, simd\_tmp.cpp, is shown. { Pyker}  generates C++ code capable of executing SIMD instructions like simd\_tmp.cpp
from the described formulas. The next section will explain the Pyker.

\subsection{Pyker}
In this DSL, code is generated by writing and compiling two definitions: the definition of variables and the definition of interaction formulas. First, let us explain the variable definition.


In this DSL, variables are classified into classes: EPI, EPJ, FORCE, and others. EPI represents particles that receive interactions, 
while EPJ represents particles that provide interactions. FORCE holds the results of the interaction calculations, and other variables 
include softening parameters and the like. To perform calculations using these variables, in C++, it would be similar to \ref{fuga}.



\begin{lstlisting}[frame=single, caption=skeleton code, label=fuga]
int kernel(double xi[][3], double xj[][3], double ai[][3], double eps2, int n){
	// (1).preprocess
	for(int i = 0;i < n;i += 4) { // (2)Increment the number of parallels
		// (3).load EPI
		// (4).Initialization of tmporary force
		for(int j = 0;j < n;j += 1) {
        // (5) load EPJ
				// (6) calculate interparticle interactions
		}
		// (7). Store calculation result in the FORCE
	}
}
\end{lstlisting}

(1) involves preparatory steps such as defining variables necessary for the calculation. (2) 
involves loading the variables of EPI. (3) is about initializing variables that temporarily
 hold the results of the interaction calculation. (4) involves performing the interaction 
 calculation and saving the results in the primary variable of force. (5) involves storing the results in the FORCE variables.
To generate this code, in variable definition, information about the class of the variable,
 its type, and its dimension is necessary. Therefore, in pyker, variable definitions are written as follows. .
The first column, if it is a class, writes the name of the class and its member variable
 name. The second column explicitly writes the dimensions of the vector if there are any, 
 specifying 3 or 4 dimensions, and the type is either 32-bit or 64-bit floating-point. The 
 third column becomes the name of the variable handled on pykg. For example, "EPI.pos vec3<F64> xi" 
 would be a member variable of EPI named pos, a 3-dimensional 64-bit floating-point variable named xi.
The definition of interaction formulas is written to accumulate the results in the FORCE variable using
 the defined variables. Primary variables necessary for the mathematical description can be newly defined 
 using previously defined variables. Available operations include basic arithmetic, sqrt, and the power 
 symbol "**". The result is stored in the FORCE variable using '='.For instance, to generate code that c
 alculates the gravitational interaction formula shown in Figure 1, it would be written in this DSL as figure1.pyker.


\begin{lstlisting}[frame=single, caption=hoge, label=fuga]
EPI.pos vec3<F64> xi
EPJ.pos vec3<F64> xj
EPJ.m F64 mass
FORCE.acc vec3<F64> ai
F64 eps2
F64 g
dr = xj - xi
ai = g * mass *  dr / sqrt(dr ** 2 + eps2) ** 3
\end{lstlisting}
\subsection{Implementation}
The process of generating code is illustrated in the following flowchart. In Parsing, the information
of the variables defined in the DSL code is converted into a hash with the variable names as keys. 
Formulas are converted into syntax trees using the sympify() method of Sympy, and a list of these 
trees is obtained. Next, Common Subexpression Elimination (CSE) is performed on the syntax trees of the converted formulas. CSE is the process 
of reducing the number of operations by pre-calculating common subexpressions in formulas and using 
their results in subsequent calculations. In type inference, the types of the primary variables in the 
formulas are inferred. Afterward, all syntax trees are converted into binary trees to align them with 
SIMD operations. Finally, the code for calculating interactions is generated by traversing the processed 
syntax trees. The following sections describe how the implementation was carried out in the order of
Parsing, CSE, type inference, conversion to binary trees, and Code generation.
\includegraphics[width=0.2\textwidth]{flowchartver3.jpg}
\subsubsection{Parsing}
In Parsing, the process is divided between variable definition and formula handling. In variable definition, 
objects with class, vector, type, and variable name are created respectively. A hash with the variable name
as the key is then made (name\_variable\_map). Formulas use the sympify() method from the Sympy library to convert
the code read as a string into a syntax tree of formulas, obtaining a list of these syntax trees (expr\_list). Next, 
Common Subexpression Elimination (CSE) is performed.
\subsubsection{CSE}

\subsubsection{type inference}
\subsubsection{convert to binary trees}

\subsubsection{code generatation of non SIMD}


\subsubsection{code generatation of SIMD}




  
\section{Experiments}

\subsection{gravity interparticle}
ほぼ同じコードで実行


\begin{lstlisting}[frame=single, caption=Nbody-kernel.pyker, label=Nbody-kernel.pyker]
  EPI vec3<F64> ri
  EPJ vec3<F64> rj
  EPJ F64 mass
  EPJ F64 eps2
  FORCE vec3<F64> ai
  rij = ri - rj
  r2 = rij * rij + eps2
  r_inv = 1.0 / sqrt(r2)
  r2_inv = r_inv * r_inv
  mr_inv = mass * r_inv
  mr3_inv = r2_inv * mr_inv
  ai = mr3_inv * rij
  \end{lstlisting}

\begin{lstlisting}[frame=single, caption=Nbody-kernel.pikg, label=Nbody-kernel.pikg]
  EPI F64vec ri:r
  EPJ F64vec rj:r
  EPJ F64 mj:m
  EPJ F64 eps2:eps
  FORCE F64vec ai:acc
  rij = ri - rj
  r2 = rij * rij + eps2
  r_inv  = rsqrt(r2)
  r2_inv = r_inv * r_inv
  mr_inv  = mj * r_inv
  mr3_inv = r2_inv * mr_inv
  ai += mr3_inv * rij
  
\end{lstlisting}




non-simd

g++ -O3 

\begin{tabular}{|l|r|r|} \hline
  N & Pyker & PIKG \\ \hline
  25000 & 6.725156 & 4.444766 \\
  10000 & 0.738918sec & 0.517324sec \\


 \\ \hline
\end{tabular}


\begin{lstlisting}[frame=single, caption=Nbody-kernel.pikg, label=Nbody-kernel.pikg]
  g++ -O3 -I ~/school/PIKG/PIKG/inc -mavx2 -mfma gravity_interparticle.cpp
  
  \end{lstlisting}



  \begin{tabular}{|l|r|r|} \hline
    N & Pyker(sec) & PIKG(sec) \\ \hline
    50000 & 5.119071 & 3.333270\\
    25000 & 1.218129 & 1.148511 \\
    10000 & 0.143777 & 0.101594 \\
    1000 &  0.003521 & 0.002168 \\ \hline
  \end{tabular}
  
% \begin{tabular}{lrr}
%   N & pyker & PIKG \\
%   10000 & 0.143777sec & 0.101594sec \\
%   x & y & z 
%  \end{tabular}

\subsection{LennardJones}

\begin{lstlisting}[frame=single, caption=LennardJones-kernel.pyker, label=LennardJones-kernel.pyker]
EPI F64 rix
EPI F64 riy
EPI F64 riz
EPJ F64 rjx
EPJ F64 rjy
EPJ F64 rjz
EPJ F64 eps

FORCE F64 fx
FORCE F64 fy
FORCE F64 fz
FORCE F64 p
dx = rix - rjx
dy = riy - rjy
dz = riz - rjz
r2 = dx * dx + dy * dy + dz * dz + eps
r2i = 1.0 / r2
r6i = r2i * r2i * r2i
f = (48.0* r6i - 24.0) * r6i * r2i
fx = f * dx
fy = f * dy
fz = f * dz
p = 4.0 * (r6i) * (r6i - 1.0)
\end{lstlisting}
\begin{lstlisting}[frame=single, caption=LennardJones-kernel.pyker, label=LennardJones-kernel.pyker]
  EPI F64 rix:rx
  EPI F64 riy:ry
  EPI F64 riz:rz
  
  EPJ F64 rjx:rx
  EPJ F64 rjy:ry
  EPJ F64 rjz:rz
  EPJ F64 eps2:eps
  
  FORCE F64 fx:fx
  FORCE F64 fy:fy
  FORCE F64 fz:fz
  FORCE F64 p:p
  
  dx  = rix - rjx
  dy = riy - rjy
  dz  = riz - rjz
  r2  = dx * dx + dy * dy + dz * dz + eps2
  r2i = 1.0 / r2
  r6i = r2i * r2i * r2i
  f = (48.0 * r6i - 24.0) * r6i * r2i
  fx += f * dx
  fy += f * dy
  fz += f * dz
  p += 4.0 * r6i*(r6i - 1.0)

\end{lstlisting}


non-simd
\begin{tabular}{|l|r|r|} \hline
  N & Pyker & PIKG \\ \hline
  50000 & 40.736862 & 11.656278 \\
  25000 & 10.662050 & 3.521253 \\
  10000 & 0.985475 & 0.442775 \\
 \\ \hline
\end{tabular}


simd
\begin{tabular}{|l|r|r|} \hline
  N & Pyker & PIKG \\ \hline
  50000 & 3.702871  & 2.838773\\
  25000 & 0.838122 & 0.9904373 \\
  10000 & 0.134388 & 0.215270 \\

 \\ \hline
\end{tabular}



\section{Conclusion}
\section{Acknowledgement}

\begin{thebibliography}{99}
\end{thebibliography}
\bibliography{biblist}


\end{document}