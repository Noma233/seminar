#include<math.h>
#include<x86intrin.h>;
int kernel(int n, double ri[][3], double rj[][3], double fr[][3], double p[], double eps){
   int i;
   int j;
   __m256d ri_tmp_v0;
   __m256d ri_tmp_v1;
   __m256d ri_tmp_v2;
   __m256d rj_tmp_v0;
   __m256d rj_tmp_v1;
   __m256d rj_tmp_v2;
   __m256d fr_tmp_v0;
   __m256d fr_tmp_v1;
   __m256d fr_tmp_v2;
   __m256d p_tmp;
   __m256d eps_tmp;
   eps_tmp = _mm256_set1_pd(eps);
   __m256d float_4_tmp;
   float_4_tmp = _mm256_set1_pd(1.0);
   __m256d float_1_tmp;
   float_1_tmp = _mm256_set1_pd(48.0);
   __m256d float_2_tmp;
   float_2_tmp = _mm256_set1_pd(24.0);
   __m256d float_3_tmp;
   float_3_tmp = _mm256_set1_pd(4.0);
   __m256d dr_tmp_v0;
   __m256d dr_tmp_v1;
   __m256d dr_tmp_v2;
   __m256d r2_tmp;
   __m256d r2i_tmp;
   __m256d f_tmp;
   __m256d x0_tmp;
   for (i = 0; i < n; i += 4) {
      int index_gather_0[4] = {0, 3, 6, 9};
      __m128i vindex_gather_0 = _mm_load_si128((const __m128i*)index_gather_0);
      ri_tmp_v0 = _mm256_i32gather_pd(&ri[i][0], vindex_gather_0, 8);
      int index_gather_1[4] = {0, 3, 6, 9};
      __m128i vindex_gather_1 = _mm_load_si128((const __m128i*)index_gather_1);
      ri_tmp_v1 = _mm256_i32gather_pd(&ri[i][1], vindex_gather_1, 8);
      int index_gather_2[4] = {0, 3, 6, 9};
      __m128i vindex_gather_2 = _mm_load_si128((const __m128i*)index_gather_2);
      ri_tmp_v2 = _mm256_i32gather_pd(&ri[i][2], vindex_gather_2, 8);
      fr_tmp_v0 = _mm256_set1_pd(0.0);
      fr_tmp_v1 = _mm256_set1_pd(0.0);
      fr_tmp_v2 = _mm256_set1_pd(0.0);
      p_tmp = _mm256_set1_pd(0.0);
      for (j = 0; j < n; j += 1) {
         rj_tmp_v0 = _mm256_set1_pd(rj[j][0]);
         rj_tmp_v1 = _mm256_set1_pd(rj[j][1]);
         rj_tmp_v2 = _mm256_set1_pd(rj[j][2]);
         dr_tmp_v0 = _mm256_sub_pd(ri_tmp_v0, rj_tmp_v0);
         dr_tmp_v1 = _mm256_sub_pd(ri_tmp_v1, rj_tmp_v1);
         dr_tmp_v2 = _mm256_sub_pd(ri_tmp_v2, rj_tmp_v2);
         r2_tmp = _mm256_add_pd(eps_tmp, _mm256_add_pd(_mm256_add_pd(_mm256_mul_pd(dr_tmp_v0, dr_tmp_v0), _mm256_mul_pd(dr_tmp_v1, dr_tmp_v1)), _mm256_mul_pd(dr_tmp_v2, dr_tmp_v2)));
         r2i_tmp = _mm256_div_pd(float_4_tmp, r2_tmp);
         x0_tmp = _mm256_mul_pd(r2i_tmp, _mm256_mul_pd(r2i_tmp, r2i_tmp));
         f_tmp = _mm256_mul_pd(_mm256_mul_pd(r2i_tmp, _mm256_mul_pd(r2i_tmp, _mm256_mul_pd(r2i_tmp, r2i_tmp))), _mm256_sub_pd( _mm256_mul_pd(float_1_tmp, x0_tmp), float_2_tmp));
         fr_tmp_v0 += _mm256_mul_pd(dr_tmp_v0, f_tmp);
         fr_tmp_v1 += _mm256_mul_pd(dr_tmp_v1, f_tmp);
         fr_tmp_v2 += _mm256_mul_pd(dr_tmp_v2, f_tmp);
         p_tmp += _mm256_mul_pd(_mm256_mul_pd(float_3_tmp, x0_tmp), _mm256_sub_pd( x0_tmp,float_4_tmp));
      };
      fr[i][0] = fr_tmp_v0[0];
      fr[i + 1][0] = fr_tmp_v0[1];
      fr[i + 2][0] = fr_tmp_v0[2];
      fr[i + 3][0] = fr_tmp_v0[3];
      fr[i][1] = fr_tmp_v1[0];
      fr[i + 1][1] = fr_tmp_v1[1];
      fr[i + 2][1] = fr_tmp_v1[2];
      fr[i + 3][1] = fr_tmp_v1[3];
      fr[i][2] = fr_tmp_v2[0];
      fr[i + 1][2] = fr_tmp_v2[1];
      fr[i + 2][2] = fr_tmp_v2[2];
      fr[i + 3][2] = fr_tmp_v2[3];
      _mm256_store_pd(&p[i], p_tmp);
   };
   return 0;
};